#!/usr/bin/env python
##########################################################################
# NSAp - Copyright (C) CEA, 2018
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
##########################################################################

# System imports
import os
import sys
import glob
import re
import argparse
import textwrap
import json
import pickle
from datetime import datetime
from argparse import RawTextHelpFormatter
import pprint
from collections import OrderedDict
import logging
if sys.version_info[0] < 3:
    import ConfigParser
else:
    import configparser as ConfigParser

# Package import
from pysap import info
from pysap.data import get_sample_data
from pysap.numerics.gridsearch import grid_search
from pysap.numerics.gridsearch import sparse_rec_condatvu
from pysap.numerics.gridsearch import sparse_rec_fista
from pysap.plugins.mri.reconstruct.utils import convert_mask_to_locations

# Third party import
import numpy as np
from numpy.random import randn
import scipy.fftpack as pfft
from modopt.math.metrics import ssim, snr, psnr, nrmse

# Parse parameters
DOC = """
LAUNCHER
========

Gridsearch the optimization parameters.

Credit: H Cherkaoui
"""

def is_directory(dirarg):
    """ Type for argparse - checks that directory exists.
    """
    if not os.path.isdir(dirarg):
        raise argparse.ArgumentError(
            "The directory '{0}' does not exist!".format(dirarg))
    return dirarg

def is_file(filepath):
    """ Check file's existence - argparse 'type' argument.
    """
    if not os.path.isfile(filepath):
        raise argparse.ArgumentError("File does not exist: %s" % filepath)
    return filepath

parser = argparse.ArgumentParser(
    prog="pysap_gridsearch",
    description=textwrap.dedent(DOC),
    formatter_class=RawTextHelpFormatter)

required = parser.add_argument_group("required arguments")
required.add_argument(
    "-o", "--output-dir",
    required=True, type=is_directory,
    help="root directoy of the results.")
required.add_argument(
    "-c", "--config-file",
    required=True, type=is_file,
    help="the gridsearch configuration file.")
parser.add_argument(
    "-v", "--verbose",
    action="store_true",
    help="increase output verbosity")
args = parser.parse_args()

# Configure the logger
if args.verbose:
    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
else:
    logging.basicConfig(stream=sys.stdout, level=logging.ERROR)
logging.info(info())


def _l2_normalize(x):
    """ Normalize x by its l2 norm.

    Parameters:
    -----------
    x: np.ndarray
        the input array.

    Return:
    -------
    x_norm: np.ndarray
        the l2 normalized array.
    """
    return x / np.linalg.norm(x)


def get_example_data(sigma=0.0, mask_type="cartesianR4", acc_factor=None):

    ref = get_sample_data("mri-slice-nifti")
    ref.data = _l2_normalize(ref.data)
    mask = get_sample_data("mri-mask")
    # Generate the subsampled kspace
    kspace_mask = pfft.ifftshift(mask.data)
    kspace = pfft.fft2(ref.data) * kspace_mask

    # Get the locations of the kspace samples
    loc = convert_mask_to_locations(kspace_mask)

    # create noise
    noise = sigma * (randn(*kspace.shape) + 1.j * randn(*kspace.shape))

    # save the noise level
    info = {"sigma": sigma}
    info["snr"] = 20.0 * np.log(np.linalg.norm(kspace) / np.linalg.norm(noise))
    info["psnr"] = 20.0*np.log(np.max(np.abs(kspace)) / np.linalg.norm(noise))

    # add noise
    kspace = kspace + noise

    # binary mask
    binary_mask = np.ones(ref.shape)

    # info
    info.update({
        "N": 512,
        "FOV(mm)": 200,
        "TE(ms)": 30,
        "TR(ms)": 550,
        "Tobs(ms)": 30.72, "Angle(degree)": 25,
        "Slice-thickness(mm)": 3,
        "Contrast": "T2*w"})
    info["mask_type"] = mask_type
    info["acc_factor"] = acc_factor

    return ref.data.astype("complex128"),\
        loc.astype("double"),\
        kspace.astype("complex128"),\
        np.rot90(np.fliplr(binary_mask)), info


def _gather_result(metric, metric_direction, list_kwargs, results):
    """ Gather the best reconstruction result.

    Parameters:
    -----------
    metric: str,
        the name of the metric, it will become a dict key in the ouput dict.
    metric_direction: bool,
        if True the higher the better the metric value is, else the lower the
        better.
    list_kwargs: list of dict,
        the list of kwargs of the gridsearch
    results: list of list,
        list of the result of the gridsearch

    Return:
    -------
    gathered_results: dict,
        the gatheres result: the best value of the metric, the best set of
        parameters, the best reconstructed data.
    """
    list_metric = []
    for res in results:
        list_metric.append(res[2][metric]["values"][-1])

    list_metric = np.array(list_metric)

    # get best runs
    if metric_direction:
        best_metric = list_metric.max()
        best_idx = list_metric.argmax()
    else:
        best_metric = list_metric.min()
        best_idx = list_metric.argmin()
    tmp_params = list_kwargs[best_idx]
    best_params = {}

    # reduce params kwargs
    best_params["max_nb_of_iter"] = tmp_params["max_nb_of_iter"]
    best_params["mu"] = tmp_params["mu"]
    try:
        best_params["sigma"] = tmp_params["sigma"]
    except KeyError:  # in case of fista run
        pass
    best_result = results[best_idx]

    return {"best_value": best_metric, "best_params": best_params,
            "best_result": best_result}


def _launch(sigma, mask_type, acc_factor, dirname, max_nb_of_iter, n_jobs,
            timeout, verbose_reconstruction, verbose_gridsearch):
    """ Launch a grid search to the specific given data.
    """
    # data loading
    res = get_example_data(sigma, mask_type, acc_factor)
    ref, loc, kspace, binmask, info = res[0], res[1], res[2], res[3], res[4]
    logging.info("Data information:\n\n{0}\n".format(pprint.pformat(info)))

    # metric declaration
    metrics = {
        "ssim": {
            "metric": ssim,
            "mapping": {"x_new": "test", "y_new": None},
            "cst_kwargs": {"ref": ref, "mask": binmask},
            "early_stopping": True},
        "psnr": {
            "metric": psnr,
            "mapping": {"x_new": "test", "y_new": None},
            "cst_kwargs": {"ref": ref, "mask": binmask},
            "early_stopping": False},
        "snr": {
            "metric": snr,
            "mapping": {"x_new": "test", "y_new": None},
            "cst_kwargs": {"ref": ref, "mask": binmask},
            "early_stopping": False},
        "nrmse": {
            "metric": nrmse,
            "mapping": {"x_new": "test", "y_new": None},
            "cst_kwargs": {"ref": ref, "mask": binmask},
            "early_stopping": False}
    }

    # principal gridsearch params grid
    # mu_list = list(np.logspace(-8, -1, 20))
    mu_list = list(np.logspace(-8, -1, 2))
    # nb_scales = [3, 4, 5]
    nb_scales = [3]
    list_wts = ["MallatWaveletTransform79Filters",
                # "UndecimatedBiOrthogonalTransform",
                # "MeyerWaveletsCompactInFourierSpace",
                # "BsplineWaveletTransformATrousAlgorithm",
                "FastCurveletTransform"]

    for wt in list_wts:

        logging.info("Using wavelet {0}".format(wt))

        # Params Condat
        params = {
            "data": kspace,
            "wavelet_name": wt,
            "samples": loc,
            "nb_scales": nb_scales,
            "mu": mu_list,
            "max_nb_of_iter": max_nb_of_iter,
            "sigma": 0.1,
            "metrics": metrics,
            "verbose": verbose_reconstruction,
        }

        # launcher the gridsearch
        list_kwargs, results = grid_search(sparse_rec_condatvu,
                                           params, n_jobs=n_jobs,
                                           verbose=verbose_gridsearch)

        # # Params FISTA
        # TODO: add an option in the configuration file to select the opt
        # algorithm
        # params = {
        #     "data": kspace,
        #     "wavelet_name": wt,
        #     "samples": loc,
        #     "nb_scales": nb_scales,
        #     "mu": mu_list,
        #     "max_nb_of_iter": max_nb_of_iter,
        #     "metrics": metrics,
        #     "verbose": verbose_reconstruction,
        # }
        # # launcher the gridsearch
        # list_kwargs, results = grid_search(sparse_rec_fista,
        #                                    params, n_jobs=n_jobs,
        #                                    verbose=verbose_gridsearch)

        # gather the best result per metric
        best_results = {
            "ssim": _gather_result(
                metric="ssim", metric_direction=True, list_kwargs=list_kwargs,
                results=results),
            "snr": _gather_result(
                metric="snr", metric_direction=True, list_kwargs=list_kwargs,
                results=results),
            "psnr": _gather_result(
                metric="psnr", metric_direction=True, list_kwargs=list_kwargs,
                results=results),
            "nrmse": _gather_result(
                metric="nrmse", metric_direction=False,
                list_kwargs=list_kwargs, results=results),
        }

        # save the gathered results
        wt_dirname = os.path.join(dirname, wt)
        if not os.path.exists(wt_dirname):
            os.makedirs(wt_dirname)
        filename = ("study__{0}_{1}_{2}__{3}.pkl").format(mask_type, sigma,
                                                          acc_factor, wt)
        filepath = os.path.join(wt_dirname, filename)
        with open(filepath, "wb") as pfile:
            pickle.dump(best_results, pfile)


# Parse the configuration & lauch the simulation
config = ConfigParser.RawConfigParser()
config.read(args.config_file)
global_params = dict(config.items("Global"))
global_params["n_jobs"] = int(global_params["n_jobs"])
global_params["timeout"] = int(global_params["timeout"])
global_params["verbose_reconstruction"] = bool(global_params[
                                                "verbose_reconstruction"])
global_params["verbose_gridsearch"] = bool(global_params[
                                                "verbose_gridsearch"])
global_params["max_nb_of_iter"] = int(global_params["max_nb_of_iter"])

global_params["verbose_gridsearch"] = True
for section in config.sections():
    if "Run" in section:
        params = dict(config.items(section))
        params.update(global_params)
        try:
            params["acc_factor"] = float(params["acc_factor"])
        except ValueError:
            params["acc_factor"] = None
        sigma_list = params["sigma"].split("[")[1].split("]")[0].split(",")
        sigma_list = [float(value) for value in sigma_list]
        params["dirname"] = os.path.join(args.output_dir,
                                         params["mask_type"])
        if not os.path.exists(params["dirname"]):
            os.mkdir(params["dirname"])

        for sigma in sigma_list:
            params["sigma"] = sigma
            _launch(**params)
